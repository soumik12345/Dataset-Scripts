{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coco_2017_setup.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBl_bs3h5auh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c845f94b-5e72-4c0d-aa4c-0284bbd83679"
      },
      "source": [
        "!mkdir val2017\n",
        "!mkdir train2017\n",
        "!mkdir test2017\n",
        "!gsutil -m rsync gs://images.cocodataset.org/val2017 val2017\n",
        "!gsutil -m rsync gs://images.cocodataset.org/train2017 train2017\n",
        "!gsutil -m rsync gs://images.cocodataset.org/test2017 test2017\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip annotations_trainval2017.zip\n",
        "!pip install ray"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket.\n",
            "ServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket.\n",
            "ServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket.\n",
            "--2020-05-15 15:15:32--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.242.220\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.242.220|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  98.1MB/s    in 2.5s    \n",
            "\n",
            "2020-05-15 15:15:35 (98.1 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n",
            "Archive:  annotations_trainval2017.zip\n",
            "  inflating: annotations/instances_train2017.json  \n",
            "  inflating: annotations/instances_val2017.json  \n",
            "  inflating: annotations/captions_train2017.json  \n",
            "  inflating: annotations/captions_val2017.json  \n",
            "  inflating: annotations/person_keypoints_train2017.json  \n",
            "  inflating: annotations/person_keypoints_val2017.json  \n",
            "Collecting ray\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/e4/f91074d5faca7532064dd8f01bf3bc6bffaf5cc81b9605c85f8b1bb6c841/ray-0.8.5-cp36-cp36m-manylinux1_x86_64.whl (21.2MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray) (2.6.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray) (1.18.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.6/dist-packages (from ray) (1.28.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray) (3.10.0)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from ray) (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray) (3.0.12)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting msgpack<1.0.0,>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/a8/e01fea81691749044a7bfd44536483a296d9c0a7ed4ec8810a229435547c/msgpack-0.6.2-cp36-cp36m-manylinux1_x86_64.whl (249kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 43.6MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/39/7eb5f98d24904e0f6d3edb505d4aa60e3ef83c0a58d6fe18244a51757247/aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 45.6MB/s \n",
            "\u001b[?25hCollecting redis<3.5.0,>=3.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/05/1fc7feedc19c123e7a95cfc9e7892eb6cdd2e5df4e9e8af6384349c1cc3d/redis-3.4.1-py2.py3-none-any.whl (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray) (3.13)\n",
            "Collecting py-spy>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/a7/ab45c9ee3c4654edda3efbd6b8e2fa4962226718a7e3e3be6e3926bf3617/py_spy-0.3.3-py2.py3-none-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio->ray) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray) (46.1.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray) (4.6.3)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (3.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (19.3.0)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Collecting multidict<5.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/95/f50352b5366e7d579e8b99631680a9e32e1b22adfa1629a8f23b1d22d5e2/multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 30.3MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/8f/0209fc5d975f839344c33c822ff2f7ef80f6b1e984673a5a68f960bfa583/yarl-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (3.6.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from idna-ssl>=1.0; python_version < \"3.7\"->aiohttp->ray) (2.9)\n",
            "Building wheels for collected packages: idna-ssl\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3162 sha256=fd1e244552453e7c99c4203499897d5cfe8f51b8c034711d5f4afb5f4835df71\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built idna-ssl\n",
            "Installing collected packages: colorama, msgpack, idna-ssl, multidict, async-timeout, yarl, aiohttp, redis, py-spy, ray\n",
            "  Found existing installation: msgpack 1.0.0\n",
            "    Uninstalling msgpack-1.0.0:\n",
            "      Successfully uninstalled msgpack-1.0.0\n",
            "Successfully installed aiohttp-3.6.2 async-timeout-3.0.1 colorama-0.4.3 idna-ssl-1.1.0 msgpack-0.6.2 multidict-4.7.6 py-spy-0.3.3 ray-0.8.5 redis-3.4.1 yarl-1.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuVQZeD65A1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import csv, io, json, os, ray"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTJz-k1o5JP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxAiflqI6CQJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3a2f53a2-f945-413a-d103-8a0d32f49204"
      },
      "source": [
        "num_train_shards = 64\n",
        "num_val_shards = 8\n",
        "ray.init()\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-15 15:16:15,235\tINFO resource_spec.py:212 -- Starting Ray with 7.23 GiB memory available for workers and up to 3.62 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2020-05-15 15:16:15,688\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZWH74pX6EUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chunkify(l, n):\n",
        "    size = len(l) // n\n",
        "    start = 0\n",
        "    results = []\n",
        "    for i in range(n - 1):\n",
        "        results.append(l[start:start + size])\n",
        "        start += size\n",
        "    results.append(l[start:])\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax7qXV0o6G__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bytes_feature(value):\n",
        "    if isinstance(value, type(tf.constant(0))):\n",
        "        value = value.numpy()\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0t9CA9Y6LHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def genreate_tfexample(anno_list):\n",
        "    filename = anno_list[0]['filename']\n",
        "    with open(filename, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "    image = Image.open(filename)\n",
        "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
        "        image_rgb = image.convert('RGB')\n",
        "        with io.BytesIO() as output:\n",
        "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
        "            content = output.getvalue()\n",
        "    width, height = image.size\n",
        "    depth = 3\n",
        "    class_ids = []\n",
        "    class_texts = []\n",
        "    bbox_xmins = []\n",
        "    bbox_ymins = []\n",
        "    bbox_xmaxs = []\n",
        "    bbox_ymaxs = []\n",
        "    for anno in anno_list:\n",
        "        class_ids.append(anno['class_id'])\n",
        "        class_texts.append(anno['class_text'].encode())\n",
        "        xmin, ymin, xmax, ymax = anno['xmin'], anno['ymin'], anno[\n",
        "            'xmax'], anno['ymax']\n",
        "        bbox_xmin, bbox_ymin, bbox_xmax, bbox_ymax = float(\n",
        "            xmin) / width, float(ymin) / height, float(xmax) / width, float(\n",
        "                ymax) / height\n",
        "        assert bbox_xmin <= 1 and bbox_xmin >= 0\n",
        "        assert bbox_ymin <= 1 and bbox_ymin >= 0\n",
        "        assert bbox_xmax <= 1 and bbox_xmax >= 0\n",
        "        assert bbox_ymax <= 1 and bbox_ymax >= 0\n",
        "        bbox_xmins.append(bbox_xmin)\n",
        "        bbox_ymins.append(bbox_ymin)\n",
        "        bbox_xmaxs.append(bbox_xmax)\n",
        "        bbox_ymaxs.append(bbox_ymax)\n",
        "    feature = {\n",
        "        'image/height':\n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
        "        'image/width':\n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
        "        'image/depth':\n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
        "        'image/object/bbox/xmin':\n",
        "        tf.train.Feature(float_list=tf.train.FloatList(value=bbox_xmins)),\n",
        "        'image/object/bbox/ymin':\n",
        "        tf.train.Feature(float_list=tf.train.FloatList(value=bbox_ymins)),\n",
        "        'image/object/bbox/xmax':\n",
        "        tf.train.Feature(float_list=tf.train.FloatList(value=bbox_xmaxs)),\n",
        "        'image/object/bbox/ymax':\n",
        "        tf.train.Feature(float_list=tf.train.FloatList(value=bbox_ymaxs)),\n",
        "        'image/object/class/label':\n",
        "        tf.train.Feature(int64_list=tf.train.Int64List(value=class_ids)),\n",
        "        'image/object/class/text':\n",
        "        tf.train.Feature(bytes_list=tf.train.BytesList(value=class_texts)),\n",
        "        'image/encoded':\n",
        "        _bytes_feature(content),\n",
        "        'image/filename':\n",
        "        _bytes_feature(os.path.basename(filename).encode())\n",
        "    }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZgUhQ7m6Oc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@ray.remote\n",
        "def build_single_tfrecord(chunk, path):\n",
        "    print('start to build tf records for ' + path)\n",
        "    with tf.io.TFRecordWriter(path) as writer:\n",
        "        for anno_list in chunk:\n",
        "            tf_example = genreate_tfexample(anno_list)\n",
        "            writer.write(tf_example.SerializeToString())\n",
        "    print('finished building tf records for ' + path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKMSkB796V3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_tf_records(annotations, total_shards, split):\n",
        "    annotations_by_image = {}\n",
        "    for annotation in annotations:\n",
        "        if annotation['filename'] in annotations_by_image:\n",
        "            annotations_by_image[annotation['filename']].append(annotation)\n",
        "        else:\n",
        "            annotations_by_image[annotation['filename']] = [annotation]\n",
        "    chunks = chunkify(list(annotations_by_image.values()), total_shards)\n",
        "    futures = [\n",
        "        # train_0001_of_0064.tfrecords\n",
        "        build_single_tfrecord.remote(\n",
        "            chunk, './tfrecords/{}_{}_of_{}.tfrecords'.format(\n",
        "                split,\n",
        "                str(i + 1).zfill(4),\n",
        "                str(total_shards).zfill(4),\n",
        "            )) for i, chunk in enumerate(chunks)\n",
        "    ]\n",
        "    ray.get(futures)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPpUaXUT6Y2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_one_annotation(anno, categories, dir):\n",
        "    category_id = int(anno['category_id'])\n",
        "    category = categories[category_id]\n",
        "    class_id = category[0]\n",
        "    if class_id < 0:\n",
        "        print('ALERT: class is {} is invalid'.format(class_id))\n",
        "    class_text = category[1]\n",
        "    bbox = anno['bbox']\n",
        "    filename = '{}/{}.jpg'.format(dir, str(anno['image_id']).rjust(12, '0'))\n",
        "    annotation = {\n",
        "        'filename': filename,\n",
        "        'class_id': class_id,\n",
        "        'class_text': class_text,\n",
        "        'xmin': float(bbox[0]),\n",
        "        'ymin': float(bbox[1]),\n",
        "        'xmax': float(bbox[0]) + float(bbox[2]),\n",
        "        'ymax': float(bbox[1]) + float(bbox[3]),\n",
        "    }\n",
        "    return annotation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ctvNjHn6esw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Start to parse annotations.')\n",
        "if not os.path.exists('./tfrecords'):\n",
        "    os.makedirs('./tfrecords')\n",
        "with open('./annotations/instances_train2017.json') as train_json:\n",
        "    train_annos = json.load(train_json)\n",
        "    train_categories = {\n",
        "        category['id']: (i, category['name'])\n",
        "        for i, category in enumerate(train_annos['categories'])\n",
        "    }\n",
        "    print(train_categories)\n",
        "    train_annotations = [\n",
        "        parse_one_annotation(anno, train_categories, './train2017')\n",
        "        for anno in train_annos['annotations']\n",
        "    ]\n",
        "    del (train_annos)\n",
        "with open('./annotations/instances_val2017.json') as val_json:\n",
        "    val_annos = json.load(val_json)\n",
        "    val_categories = {\n",
        "        category['id']: (i, category['name'])\n",
        "        for i, category in enumerate(val_annos['categories'])\n",
        "    }\n",
        "    print(val_categories)\n",
        "    val_annotations = [\n",
        "        parse_one_annotation(anno, val_categories, './val2017')\n",
        "        for anno in val_annos['annotations']\n",
        "    ]\n",
        "    del (val_annos)\n",
        "print('Start to build TF Records.')\n",
        "build_tf_records(train_annotations, num_train_shards, 'train')\n",
        "build_tf_records(val_annotations, num_val_shards, 'val')\n",
        "print('Successfully wrote {} annotations to TF Records.'.format(\n",
        "    len(train_annotations) + len(val_annotations)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}